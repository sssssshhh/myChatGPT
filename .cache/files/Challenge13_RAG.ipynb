{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1830, which is longer than the specified 600\n",
      "Created a chunk of size 1700, which is longer than the specified 600\n",
      "Created a chunk of size 2301, which is longer than the specified 600\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/soo/Downloads/git/myChatGPT/Challenge13_RAG.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/soo/Downloads/git/myChatGPT/Challenge13_RAG.ipynb#W0sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_memory\u001b[39m(_):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/soo/Downloads/git/myChatGPT/Challenge13_RAG.ipynb#W0sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m memory\u001b[39m.\u001b[39mload_memory_variables({})[\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/soo/Downloads/git/myChatGPT/Challenge13_RAG.ipynb#W0sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m map_chain \u001b[39m=\u001b[39m { \u001b[39m\"\u001b[39;49m\u001b[39mdocuments\u001b[39;49m\u001b[39m\"\u001b[39;49m: retriver, \u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m] } \u001b[39m|\u001b[39;49m RunnableLambda(map_docs) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/soo/Downloads/git/myChatGPT/Challenge13_RAG.ipynb#W0sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m chain \u001b[39m=\u001b[39m RunnablePassthrough\u001b[39m.\u001b[39massign(chat_history\u001b[39m=\u001b[39mload_memory) \u001b[39m|\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m: map_chain,\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m]} \u001b[39m|\u001b[39m final_prompt \u001b[39m|\u001b[39m llm\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/soo/Downloads/git/myChatGPT/Challenge13_RAG.ipynb#W0sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke_chain\u001b[39m(question):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/soo/Downloads/git/myChatGPT/Challenge13_RAG.ipynb#W0sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m    \u001b[39m# print(type(chain.invoke({\"question\":question})))\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git/myChatGPT/env/lib/python3.11/site-packages/langchain/schema/runnable/base.py:275\u001b[0m, in \u001b[0;36mRunnable.__ror__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__ror__\u001b[39m(\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    267\u001b[0m     other: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     ],\n\u001b[1;32m    273\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Runnable[Other, Output]:\n\u001b[1;32m    274\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableSequence(first\u001b[39m=\u001b[39mcoerce_to_runnable(other), last\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/git/myChatGPT/env/lib/python3.11/site-packages/langchain/schema/runnable/base.py:2783\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   2781\u001b[0m     \u001b[39mreturn\u001b[39;00m RunnableLambda(cast(Callable[[Input], Output], thing))\n\u001b[1;32m   2782\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(thing, \u001b[39mdict\u001b[39m):\n\u001b[0;32m-> 2783\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[1;32m   2784\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2785\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   2786\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2787\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2788\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/git/myChatGPT/env/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1622\u001b[0m, in \u001b[0;36mRunnableParallel.__init__\u001b[0;34m(self, _RunnableParallel__steps, **kwargs)\u001b[0m\n\u001b[1;32m   1619\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   1620\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1621\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m-> 1622\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;49;00m key, r \u001b[39min\u001b[39;49;00m merged\u001b[39m.\u001b[39;49mitems()}\n\u001b[1;32m   1623\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/git/myChatGPT/env/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1622\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1619\u001b[0m merged \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__steps} \u001b[39mif\u001b[39;00m __steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   1620\u001b[0m merged\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1621\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m-> 1622\u001b[0m     steps\u001b[39m=\u001b[39m{key: coerce_to_runnable(r) \u001b[39mfor\u001b[39;00m key, r \u001b[39min\u001b[39;00m merged\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m   1623\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/git/myChatGPT/env/lib/python3.11/site-packages/langchain/schema/runnable/base.py:2785\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   2783\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Runnable[Input, Output], RunnableParallel(thing))\n\u001b[1;32m   2784\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2785\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   2786\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a Runnable, callable or dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2787\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead got an unsupported type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(thing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2788\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"question\":\"{question}\"}, {\"result\": \"{result}\"}\n",
    ")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"Winston\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=50\n",
    "    )\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "    \"\"\"\n",
    "    Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "    Return any relevant text verbatim.\n",
    "    -----\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    ),\n",
    "    (\"human\",\"{question}\"),\n",
    "])\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# context = extracted parts of a long document. 도큐멘트의 요약본\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "    \"\"\"\n",
    "    Given the following extracted parts of a long document and a question,create a final answer.\n",
    "    If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "    ---\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{question}\"),\n",
    "    ])\n",
    "\n",
    "def map_docs(inputs):\n",
    "    print(\"map_docs\")\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    return \"¥n¥n\".join(map_doc_chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": question\n",
    "        }).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "map_chain = { \"documents\": retriver, \"question\": [\"question\"] } | RunnableLambda(map_docs) \n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | {\"context\": map_chain,\"question\": [\"question\"]} | final_prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "   result = chain.invoke({\"question\":question})\n",
    "   memory.save_context({\"question\":question}, {\"result\": result.content})\n",
    "\n",
    "invoke_chain(\"Is Aaronson guilty?\")\n",
    "#invoke_chain(\"What message did he write in the table?\")\n",
    "#invoke_chain(\"Who is Julia?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
